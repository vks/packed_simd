<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Rust SIMD Performance Guide</title>
        
        <meta name="robots" content="noindex" />
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="This book describes how to write performant SIMD code in Rust.">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        <link rel="icon" href="favicon.svg">
        
        
        <link rel="shortcut icon" href="favicon.png">
        
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        
        <link rel="stylesheet" href="./src/ascii.css">
        

        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="introduction.html">Introduction</a></li><li class="chapter-item expanded "><a href="float-math/fp.html"><strong aria-hidden="true">1.</strong> Floating-point Math</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="float-math/svml.html"><strong aria-hidden="true">1.1.</strong> Short-vector Math Library</a></li><li class="chapter-item expanded "><a href="float-math/approx.html"><strong aria-hidden="true">1.2.</strong> Approximate functions</a></li><li class="chapter-item expanded "><a href="float-math/fma.html"><strong aria-hidden="true">1.3.</strong> Fused multiply-accumulate</a></li></ol></li><li class="chapter-item expanded "><a href="target-feature/features.html"><strong aria-hidden="true">2.</strong> Target features</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="target-feature/rustflags.html"><strong aria-hidden="true">2.1.</strong> Using RUSTFLAGS</a></li><li class="chapter-item expanded "><a href="target-feature/attribute.html"><strong aria-hidden="true">2.2.</strong> Using the target_feature attribute</a></li><li class="chapter-item expanded "><a href="target-feature/inlining.html"><strong aria-hidden="true">2.3.</strong> Interaction with inlining</a></li><li class="chapter-item expanded "><a href="target-feature/runtime.html"><strong aria-hidden="true">2.4.</strong> Detecting features at runtime</a></li></ol></li><li class="chapter-item expanded "><a href="bound_checks.html"><strong aria-hidden="true">3.</strong> Bounds checking</a></li><li class="chapter-item expanded "><a href="vert-hor-ops.html"><strong aria-hidden="true">4.</strong> Vertical and horizontal operations</a></li><li class="chapter-item expanded "><a href="prof/profiling.html"><strong aria-hidden="true">5.</strong> Performance profiling</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="prof/linux.html"><strong aria-hidden="true">5.1.</strong> Profiling on Linux</a></li><li class="chapter-item expanded "><a href="prof/mca.html"><strong aria-hidden="true">5.2.</strong> Using machine code analyzers</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title">Rust SIMD Performance Guide</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1><a class="header" href="#introduction" id="introduction">Introduction</a></h1>
<h2><a class="header" href="#what-is-simd" id="what-is-simd">What is SIMD</a></h2>
<!-- TODO:
describe what SIMD is, which algorithms can benefit from it,
give usage examples
-->
<h2><a class="header" href="#history-of-simd-in-rust" id="history-of-simd-in-rust">History of SIMD in Rust</a></h2>
<!-- TODO:
discuss history of unstable std::simd,
stabilization of std::arch, etc.
-->
<h2><a class="header" href="#discover-packed_simd" id="discover-packed_simd">Discover packed_simd</a></h2>
<!-- TODO: describe scope of this project -->
<p>Writing fast and portable SIMD algorithms using <code>packed_simd</code> is, unfortunately,
not trivial. There are many pitfals that one should be aware of, and some idioms
that help avoid those pitfalls.</p>
<p>This book attempts to document these best practices and provides practical examples
on how to apply the tips to <em>your</em> code.</p>
<h1><a class="header" href="#floating-point-math" id="floating-point-math">Floating-point math</a></h1>
<p>This chapter contains information pertaining to working with floating-point numbers.</p>
<h1><a class="header" href="#short-vector-math-library" id="short-vector-math-library">Short Vector Math Library</a></h1>
<!-- TODO:
Explain how is short-vector math performed by default (just scalarized libm calls).

Explain how to enable `sleef`, etc.
-->
<h1><a class="header" href="#approximate-functions" id="approximate-functions">Approximate functions</a></h1>
<!-- TODO:

Explain that they exists, that they are often _much_ faster, how to use them,
that people should check whether the error is good enough for their
applications. Explain that this error is currently unstable and might change.
-->
<h1><a class="header" href="#fused-multiply-add" id="fused-multiply-add">Fused Multiply Add</a></h1>
<!-- TODO:
Explain that this is a compound operation, infinite precision, difference
between `mul_add` and `mul_adde`, that LLVM cannot do this by itself, etc.
-->
<h1><a class="header" href="#enabling-target-features" id="enabling-target-features">Enabling target features</a></h1>
<p>Not all processors of a certain architecture will have SIMD processing units,
and using a SIMD instruction which is not supported will trigger undefined behavior.</p>
<p>To allow building safe, portable programs, the Rust compiler will <strong>not</strong>, by default,
generate any sort of vector instructions, unless it can statically determine
they are supported. For example, on AMD64, SSE2 support is architecturally guaranteed.
The <code>x86_64-apple-darwin</code> target enables up to SSSE3. The get a defintive list of
which features are enabled by default on various platforms, refer to the target
specifications <a href="https://github.com/rust-lang/rust/tree/master/src/librustc_target/spec">in the compiler's source code</a>.</p>
<h1><a class="header" href="#using-rustflags" id="using-rustflags">Using RUSTFLAGS</a></h1>
<p>One of the easiest ways to benefit from SIMD is to allow the compiler
to generate code using certain vector instruction extensions.</p>
<p>The environment variable <code>RUSTFLAGS</code> can be used to pass options for code
generation to the Rust compiler. These flags will affect <strong>all</strong> compiled crates.</p>
<p>There are two flags which can be used to enable specific vector extensions:</p>
<h2><a class="header" href="#target-feature" id="target-feature">target-feature</a></h2>
<ul>
<li>
<p>Syntax: <code>-C target-feature=&lt;features&gt;</code></p>
</li>
<li>
<p>Provides the compiler with a comma-separated set of instruction extensions
to enable.</p>
<p><strong>Example</strong>: Use <code>-C target-features=+sse3,+avx</code> to enable generating instructions
for <a href="https://en.wikipedia.org/wiki/SSE3">Streaming SIMD Extensions 3</a> and
<a href="https://en.wikipedia.org/wiki/Advanced_Vector_Extensions">Advanced Vector Extensions</a>.</p>
</li>
<li>
<p>To list target triples for all targets supported by Rust, use:</p>
<pre><code class="language-sh">rustc --print target-list
</code></pre>
</li>
<li>
<p>To list all support target features for a certain target triple, use:</p>
<pre><code class="language-sh">rustc --target=${TRIPLE} --print target-features
</code></pre>
</li>
<li>
<p>Note that all CPU features are independent, and will have to be enabled individually.</p>
<p><strong>Example</strong>: Setting <code>-C target-features=+avx2</code> will <em>not</em> enable <code>fma</code>, even though
all CPUs which support AVX2 also support FMA. To enable both, one has to use
<code>-C target-features=+avx2,+fma</code></p>
</li>
<li>
<p>Some features also depend on other features, which need to be enabled for the
target instructions to be generated.</p>
<p><strong>Example</strong>: Unless <code>v7</code> is specified as the target CPU (see below), to enable
NEON on ARM it is necessary to use <code>-C target-feature=+v7,+neon</code>.</p>
</li>
</ul>
<h2><a class="header" href="#target-cpu" id="target-cpu">target-cpu</a></h2>
<ul>
<li>
<p>Syntax: <code>-C target-cpu=&lt;cpu&gt;</code></p>
</li>
<li>
<p>Sets the identifier of a CPU family / model for which to build and optimize the code.</p>
<p><strong>Example</strong>: <code>RUSTFLAGS='-C target-cpu=cortex-a75'</code></p>
</li>
<li>
<p>To list all supported target CPUs for a certain target triple, use:</p>
<pre><code class="language-sh">rustc --target=${TRIPLE} --print target-cpus
</code></pre>
<p><strong>Example</strong>:</p>
<pre><code class="language-sh">rustc --target=i686-pc-windows-msvc --print target-cpus
</code></pre>
</li>
<li>
<p>The compiler will translate this into a list of target features. Therefore,
individual feature checks (<code>#[cfg(target_feature = &quot;...&quot;)]</code>) will still
work properly.</p>
</li>
<li>
<p>It will cause the code generator to optimize the generated code for that
specific CPU model.</p>
</li>
<li>
<p>Using <code>native</code> as the CPU model will cause Rust to generate and optimize code
for the CPU running the compiler. It is useful when building programs which you
plan to only use locally. This should never be used when the generated programs
are meant to be run on other computers, such as when packaging for distribution
or cross-compiling.</p>
</li>
</ul>
<h1><a class="header" href="#the-target_feature-attribute" id="the-target_feature-attribute">The <code>target_feature</code> attribute</a></h1>
<!-- TODO:
Explain the `#[target_feature]` attribute
-->
<h1><a class="header" href="#inlining" id="inlining">Inlining</a></h1>
<!-- TODO:
Explain how the `#[target_feature]` attribute interacts with inlining
-->
<h1><a class="header" href="#detecting-host-features-at-runtime" id="detecting-host-features-at-runtime">Detecting host features at runtime</a></h1>
<!-- TODO:
Explain cost (how it works).
-->
<h1><a class="header" href="#bounds-checking" id="bounds-checking">Bounds checking</a></h1>
<p>Reading and writing packed vectors to/from slices is checked by default.
Independently of the configuration options used, the safe functions:</p>
<ul>
<li><code>Simd&lt;[T; N]&gt;::from_slice_aligned(&amp; s[..])</code></li>
<li><code>Simd&lt;[T; N]&gt;::write_to_slice_aligned(&amp;mut s[..])</code></li>
</ul>
<p>always check that:</p>
<ul>
<li>the slice is big enough to hold the vector</li>
<li>the slice is suitably aligned to perform an aligned load/store for a <code>Simd&lt;[T; N]&gt;</code> (this alignment is often much larger than that of <code>T</code>).</li>
</ul>
<p>There are <code>_unaligned</code> versions that use unaligned load and stores, as well as
<code>unsafe</code> <code>_unchecked</code> that do not perform any checks iff <code>debug-assertions = false</code> / <code>debug = false</code>. That is, the <code>_unchecked</code> methods do still assert size
and alignment in debug builds and could also do so in release builds depending
on the configuration options.</p>
<p>These assertions do often significantly impact performance and you should be
aware of them.</p>
<h1><a class="header" href="#vertical-and-horizontal-operations" id="vertical-and-horizontal-operations">Vertical and horizontal operations</a></h1>
<p>In SIMD terminology, each vector has a certain &quot;width&quot; (number of lanes).
A vector processor is able to perform two kinds of operations on a vector:</p>
<ul>
<li>Vertical operations:
operate on two vectors of the same width, result has same width</li>
</ul>
<p><strong>Example</strong>: vertical addition of two <code>f32x4</code> vectors</p>
<pre><code>  %0     == | 2 | -3.5 |  0 | 7 |
              +     +     +   +
  %1     == | 4 |  1.5 | -1 | 0 |
              =     =     =   =
%0 + %1  == | 6 |  -2  | -1 | 7 |
</code></pre>
<ul>
<li>Horizontal operations:
reduce the elements of two vectors in some way,
the result's elements combine information from the two original ones</li>
</ul>
<p><strong>Example</strong>: horizontal addition of two <code>u64x2</code> vectors</p>
<pre><code>  %0     == | 1 |  3 |
              └─+───┘
                └───────┐
                        │
  %1     == | 4 | -1 |  │
              └─+──┘    │
                └───┐   │
                    │   │
              ┌─────│───┘
              ▼     ▼
%0 + %1  == | 4 |   3 |
</code></pre>
<h2><a class="header" href="#performance-consideration-of-horizontal-operations" id="performance-consideration-of-horizontal-operations">Performance consideration of horizontal operations</a></h2>
<p>The result of vertical operations, like vector negation: <code>-a</code>, for a given lane,
does not depend on the result of the operation for the other lanes. The result
of horizontal operations, like the vector <code>sum</code> reduction: <code>a.sum()</code>, depends on
the value of all vector lanes.</p>
<p>In virtually all architectures vertical operations are fast, while horizontal
operations are, by comparison, very slow.</p>
<p>Consider the following two functions for computing the sum of all <code>f32</code> values
in a slice:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn fast_sum(x: &amp;[f32]) -&gt; f32 {
    assert!(x.len() % 4 == 0);
    let mut sum = f32x4::splat(0.); // [0., 0., 0., 0.]
    for i in (0..x.len()).step_by(4) {
        sum += f32x4::from_slice_unaligned(&amp;x[i..]);
    }
    sum.sum()
}

fn slow_sum(x: &amp;[f32]) -&gt; f32 {
    assert!(x.len() % 4 == 0);
    let mut sum: f32 = 0.;
    for i in (0..x.len()).step_by(4) {
        sum += f32x4::from_slice_unaligned(&amp;x[i..]).sum();
    }
    sum
}
<span class="boring">}
</span></code></pre></pre>
<p>The inner loop over the slice is where the bulk of the work actually happens.
There, the <code>fast_sum</code> function perform vertical operations into a vector, doing
a single horizontal reduction at the end, while the <code>slow_sum</code> function performs
horizontal vector operations inside of the loop.</p>
<p>On all widely-used architectures, <code>fast_sum</code> is a large constant factor faster
than <code>slow_sum</code>. You can run the <a href="">slice_sum</a> example and see for yourself. On
the particular machine tested there the algorithm using the horizontal vector
addition is 2.7x slower than the one using vertical vector operations!</p>
<h1><a class="header" href="#performance-profiling" id="performance-profiling">Performance profiling</a></h1>
<p>While the rest of the book provides practical advice on how to improve the performance
of SIMD code, this chapter is dedicated to <a href="https://en.wikipedia.org/wiki/Profiling_(computer_programming)"><strong>performance profiling</strong></a>.
Profiling consists of recording a program's execution in order to identify program
hotspots.</p>
<p><strong>Important</strong>: most profilers require debug information in order to accurately
link the program hotspots back to the corresponding source code lines. Rust will
disable debug info generation by default for optimized builds, but you can change
that <a href="https://doc.rust-lang.org/cargo/reference/manifest.html#the-profile-sections">in your <code>Cargo.toml</code></a>.</p>
<h1><a class="header" href="#performance-profiling-on-linux" id="performance-profiling-on-linux">Performance profiling on Linux</a></h1>
<h2><a class="header" href="#using-perf" id="using-perf">Using <code>perf</code></a></h2>
<p><a href="https://perf.wiki.kernel.org/">perf</a> is the most powerful performance profiler
for Linux, featuring support for various hardware Performance Monitoring Units,
as well as integration with the kernel's performance events framework.</p>
<p>We will only look at how can the <code>perf</code> command can be used to profile SIMD code.
Full system profiling is outside of the scope of this book.</p>
<h3><a class="header" href="#recording" id="recording">Recording</a></h3>
<p>The first step is to record a program's execution during an average workload.
It helps if you can isolate the parts of your program which have performance
issues, and set up a benchmark which can be easily (re)run.</p>
<p>Build the benchmark binary in release mode, after having enabled debug info:</p>
<pre><code class="language-sh">$ cargo build --release
Finished release [optimized + debuginfo] target(s) in 0.02s
</code></pre>
<p>Then use the <code>perf record</code> subcommand:</p>
<pre><code class="language-sh">$ perf record --call-graph=dwarf ./target/release/my-program
[ perf record: Woken up 10 times to write data ]
[ perf record: Captured and wrote 2,356 MB perf.data (292 samples) ]
</code></pre>
<p>Instead of using <code>--call-graph=dwarf</code>, which can become pretty slow, you can use
<code>--call-graph=lbr</code> if you have a processor with support for Last Branch Record
(i.e. Intel Haswell and newer).</p>
<p><code>perf</code> will, by default, record the count of CPU cycles it takes to execute
various parts of your program. You can use the <code>-e</code> command line option
to enable other performance events, such as <code>cache-misses</code>. Use <code>perf list</code>
to get a list of all hardware counters supported by your CPU.</p>
<h3><a class="header" href="#viewing-the-report" id="viewing-the-report">Viewing the report</a></h3>
<p>The next step is getting a bird's eye view of the program's execution.
<code>perf</code> provides a <code>ncurses</code>-based interface which will get you started.</p>
<p>Use <code>perf report</code> to open a visualization of your program's performance:</p>
<pre><code class="language-sh">perf report --hierarchy -M intel
</code></pre>
<p><code>--hierarchy</code> will display a tree-like structure of where your program spent
most of its time. <code>-M intel</code> enables disassembly output with Intel syntax, which
is subjectively more readable than the default AT&amp;T syntax.</p>
<p>Here is the output from profiling the <code>nbody</code> benchmark:</p>
<pre><code>- 100,00% nbody
  - 94,18% nbody
    + 93,48% [.] nbody_lib::simd::advance
    + 0,70% [.] nbody_lib::run
    + 5,06% libc-2.28.so
</code></pre>
<p>If you move with the arrow keys to any node in the tree, you can the press <code>a</code>
to have <code>perf</code> <em>annotate</em> that node. This means it will:</p>
<ul>
<li>
<p>disassemble the function</p>
</li>
<li>
<p>associate every instruction with the percentage of time which was spent executing it</p>
</li>
<li>
<p>interleaves the disassembly with the source code,
assuming it found the debug symbols
(you can use <code>s</code> to toggle this behaviour)</p>
</li>
</ul>
<p><code>perf</code> will, by default, open the instruction which it identified as being the
hottest spot in the function:</p>
<pre><code>0,76  │ movapd xmm2,xmm0
0,38  │ movhlps xmm2,xmm0
      │ addpd  xmm2,xmm0
      │ unpcklpd xmm1,xmm2
12,50 │ sqrtpd xmm0,xmm1
1,52  │ mulpd  xmm0,xmm1
</code></pre>
<p>In this case, <code>sqrtpd</code> will be highlighted in red, since that's the instruction
which the CPU spends most of its time executing.</p>
<h2><a class="header" href="#using-valgrind" id="using-valgrind">Using Valgrind</a></h2>
<p>Valgrind is a set of tools which initially helped C/C++ programmers find unsafe
memory accesses in their code. Nowadays the project also has</p>
<ul>
<li>
<p>a heap profiler called <code>massif</code></p>
</li>
<li>
<p>a cache utilization profiler called <code>cachegrind</code></p>
</li>
<li>
<p>a call-graph performance profiler called <code>callgrind</code></p>
</li>
</ul>
<!--
TODO: explain valgrind's dynamic binary translation, warn about massive
slowdown, talk about `kcachegrind` for a GUI
-->
<h1><a class="header" href="#machine-code-analysis-tools" id="machine-code-analysis-tools">Machine code analysis tools</a></h1>
<h2><a class="header" href="#the-microarchitecture-of-modern-cpus" id="the-microarchitecture-of-modern-cpus">The microarchitecture of modern CPUs</a></h2>
<p>While you might have heard of Instruction Set Architectures, such as <code>x86</code> or
<code>arm</code> or <code>mips</code>, the term <em>microarchitecture</em> (also written here as <em>µ-arch</em>),
refers to the internal details of an actual family of CPUs, such as Intel's
<em>Haswell</em> or AMD's <em>Jaguar</em>.</p>
<p>Replacing scalar code with SIMD code will improve performance on all CPUs
supporting the required vector extensions.
However, due to microarchitectural differences, the actual speed-up at
runtime might vary.</p>
<p><strong>Example</strong>: a simple example arises when optimizing for AMD K8 CPUs.
The assembly generated for an empty function should look like this:</p>
<pre><code class="language-asm">nop
ret
</code></pre>
<p>The <code>nop</code> is used to align the <code>ret</code> instruction for better performance.
However, the compiler will actually generated the following code:</p>
<pre><code class="language-asm">repz ret
</code></pre>
<p>The <code>repz</code> instruction will repeat the following instruction until a certain
condition. Of course, in this situation, the function will simply immediately
return, and the <code>ret</code> instruction is still aligned.
However, AMD K8's branch predictor performs better with the latter code.</p>
<p>For those looking to absolutely maximize performance for a certain target µ-arch,
you will have to read some CPU manuals, or ask the compiler to do it for you
with <code>-C target-cpu</code>.</p>
<h3><a class="header" href="#summary-of-cpu-internals" id="summary-of-cpu-internals">Summary of CPU internals</a></h3>
<p>Modern processors are able to execute instructions out-of-order for better performance,
by utilizing tricks such as <a href="https://en.wikipedia.org/wiki/Branch_predictor">branch prediction</a>, <a href="https://en.wikipedia.org/wiki/Instruction_pipelining">instruction pipelining</a>,
or <a href="https://en.wikipedia.org/wiki/Superscalar_processor">superscalar execution</a>.</p>
<p>SIMD instructions are also subject to these optimizations, meaning it can get pretty
difficult to determine where the slowdown happens.
For example, if the profiler reports a store operation is slow, one of two things
could be happening:</p>
<ul>
<li>
<p>the store is limited by the CPU's memory bandwidth, which is actually an ideal
scenario, all things considered;</p>
</li>
<li>
<p>memory bandwidth is nowhere near its peak, but the value to be stored is at the
end of a long chain of operations, and this store is where the profiler
encountered the pipeline stall;</p>
</li>
</ul>
<p>Since most profilers are simple tools which don't understand the subtleties of
instruction scheduling, you</p>
<h2><a class="header" href="#analyzing-the-machine-code" id="analyzing-the-machine-code">Analyzing the machine code</a></h2>
<p>Certain tools have knowledge of internal CPU microarchitecture, i.e. they know</p>
<ul>
<li>
<p>how many physical <a href="https://en.wikipedia.org/wiki/Register_file">register files</a> a CPU actually has</p>
</li>
<li>
<p>what is the latency / throughtput of an instruction</p>
</li>
<li>
<p>what <a href="https://en.wikipedia.org/wiki/Micro-operation">µ-ops</a> are generated for a set of instructions</p>
</li>
</ul>
<p>and many other architectural details.</p>
<p>These tools are therefore able to provide accurate information as to why some
instructions are inefficient, and where the bottleneck is.</p>
<p>The disadvantage is that the output of these tools requires advanced knowledge
of the target architecture to understand, i.e. they <strong>cannot</strong> point out what
the cause of the issue is explicitly.</p>
<h2><a class="header" href="#intels-architecture-code-analyzer-iaca" id="intels-architecture-code-analyzer-iaca">Intel's Architecture Code Analyzer (IACA)</a></h2>
<p><a href="https://software.intel.com/en-us/articles/intel-architecture-code-analyzer">IACA</a> is a free tool offered by Intel for analyzing the performance of various
computational kernels.</p>
<p>Being a proprietary, closed source tool, it <em>only</em> supports Intel's µ-arches.</p>
<h2><a class="header" href="#llvm-mca" id="llvm-mca">llvm-mca</a></h2>
<!--
TODO: once LLVM 7 gets released, write a chapter on using llvm-mca
with SIMD disassembly.
-->

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
        
        

    </body>
</html>
